# 🚀 Awesome-LLM-Centric-Molecular-Discovery
A collection of AWESOME things about Molecule Discovery with LLMs.

🤗 Contributions to update new resources and articles are very welcome!

## ❖ Contents 
- [Molecule Generation](#molecule-generation)
- [Molecule Optimization](#molecule-optimization)
- [Datasets](#-datasets)
- [Evaluation Metrics](#-evaluation-metrics)

 
## Molecule Generation

|Name|Year|Category|Paper|Code|
| :------------ |:---------------: |:---------------:| :---------------| :---------------|
| **LLM4GraphGen** | arXiv 2024.04 | In-Context Learning  | [Exploring the Potential of Large Language Models in Graph Generation](https://arxiv.org/abs/2403.14358) | [Code](https://github.com/SitaoLuan/LLM4Graph) |
| **MolReGPT** | TKDE | In-Context Learning  | [MolReGPT: Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective](https://arxiv.org/pdf/2306.06615) | [Code](https://github.com/phenixace/MolReGPT) |
| **FrontierX** | arXiv 2024.08 | In-Context Learning | [Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design](https://arxiv.org/pdf/2408.11866) | [N/A]) |
| **ICMA** | arXiv 2024.03 | Surpevised Fine-Tuning  | [Large Language Models are In-Context Molecule Learners](https://arxiv.org/pdf/2403.04197) | [N/A] |
| **Mol-instructions** | ICLR 2024 | Surpevised Fine-Tuning | [MOL-INSTRUCTIONS: A LARGE-SCALE BIOMOLECULAR INSTRUCTION DATASET FOR LLMS](https://arxiv.org/pdf/2306.08018) | [Code](https://github.com/zjunlp/Mol-Instructions) |
| **LlaSMol** | COLM 2024 | Supervised Fine-Tuning | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/pdf/2403.04197) | [Code](https://osu-nlp-group.github.io/LLM4Chem/) |
| **ChemLLM** | arXiv 2024.04 | Supervised Fine-Tuning | [ChemLLM: A Chemical Large Language Model](https://arxiv.org/pdf/2402.06852) | [Code](https://huggingface.co/AI4Chem/ChemLLM-7B-Chat/tree/main) |
| **MolReFlect** | arXiv 2024.11 | Supervised Fine-Tuning | [MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts](https://arxiv.org/pdf/2411.14721) | [N/A] |
| **ChatMol** | arXiv 2025.02 | Supervised Fine-Tuning | [ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model](https://arxiv.org/pdf/2502.19794) | [Code](https://github.com/ChatMol/ChatMol) |
| **PEIT-LLM**   | arXiv 2024.12 | Supervised Fine-Tuning | [Property Enhanced Instruction Tuning for Multi-task Molecule Generation with LLMs](https://arxiv.org/pdf/2412.18084) | [Code](https://github.com/chenlong164/PEIT) |
| **NatureLM**   | arXiv 2025.02 | Supervised Fine-Tuning | [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/pdf/2502.07527) | [Code](https://naturelm.github.io/) |
| **SynLlama**   | arXiv 2025.03 | Supervised Fine-Tuning | [SynLlama: Generating Synthesizable Molecules and Analogs with LLMs](https://arxiv.org/pdf/2503.12602) |[N/A]|
| **TOMG-Bench** | arXiv 2024.12 | Supervised Fine-Tuning | [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/pdf/2412.14642) | [Code](https://github.com/phenixace/TOMG-Bench) |
| **UniMoT** | arXiv 2024.08 | Supervised Fine-Tuning | [UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation](https://arxiv.org/pdf/2408.00863) | [Code](https://uni-mot.github.io/) |
| **Div-SFT** | arXiv 2024.10 | Preference Tuning | [Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity](https://arxiv.org/pdf/2410.03138) | [N/A] |
| **Mol-MoE** | arXiv 2025.02 | Preference Tuning | [Mol-MoE: Training Preference-Guided Routers for Molecule Generation](https://arxiv.org/pdf/2502.05633) | [Code](https://github.com/ddidacus/mol-moe) |
| **SmileyLLama**| NeurIPS2024 Workshop | Preference Tuning | [SmileyLlama: Modifying LLMs for Directed Chemical Space Exploration](https://arxiv.org/pdf/2409.02231) |[N/A] |
| **ALMol** | ACL2024 Workshop | Preference Tuning | [ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation](https://arxiv.org/abs/2405.08619) |[N/A]|
| **Less for More** | arXiv 2024.05 | Preference Tuning | [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/pdf/2405.13984) | [N/A] |
| **Mol-LLM** | arXiv 2025.02 | Preference Tuning | [Mol-llm:Generalist molecular llm with improved graph utilization](https://arxiv.org/pdf/2502.02810)|[N/A]|






## Molecule Optimization

|Name|Year|Category|Paper|Code|
| :------------ |:---------------: |:---------------:| :---------------| :---------------| 
| **MOLLEO** | ICLR 2025  | Zero-Shot Prompting | [EFFICIENT EVOLUTIONARY SEARCH OVER CHEMICAL SPACE WITH LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=awWiNvQwf3) | [Code](https://github.com/zoom-wang112358/MOLLEO) |
| **LLM-MDE** | JCIM 2024 | Zero-Shot Prompting | [Large Language Models as Molecular Design Engines](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.4c01396) | [N/A] |
| **CIDD** | arXiv 2025.03 | In-Context Learning | [Pushing the boundaries of Structure-Based Drug Design through Collaborative Intelligence Drug Design](https://arxiv.org/pdf/2503.01376) | [N/A] |
| **LLM-EO** | arXiv 2025.03 | In-Context Learning | [Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models](https://arxiv.org/pdf/2410.18136)| [Code](https://github.com/deepprinciple/llmeo) |
| **MOLLM** | arXiv 2025.03 | In-Context Learning | [MOLLM: Multi-Objective Large Language Model for Molecular Design – Optimizing with Experts](https://arxiv.org/pdf/2503.01376) | [N/A] |
| **ChatDrug** | ICLR 2024 | In-Context Learning  | [Conversational Drug Editing Using Retrieval and Domain Feedback](https://openreview.net/pdf?id=yRrPfKyJQ2) | [Code](https://github.com/chao1224/ChatDrug) |
| **RE²DF** | arXiv 2024.10 | In-Context Learning | [Utilizing Large Language Models in an Iterative Paradigm with Domain Feedback for Molecule Optimization](https://arxiv.org/pdf/2410.13147) | [Code](https://github.com/lhkhiem28/Re2DF) |
| **BOPRO** | ICLR 2025 | In-Context Learning | [Searching for Optimal Solutions with LLMs via Bayesian Optimization](https://openreview.net/pdf?id=aVfDrl7xDV) | [Code](https://github.com/amazon-science/BOPRO-ICLR-2025) |
| **MultiMol** | arXiv 2025.03 | Supervised Fine-Tuning | [Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization](https://arxiv.org/html/2503.03503) | [Code](https://github.com/jiajunyu1999/LLM4Drug) |
| **DrugAssist** | Briefings in Bioinformatics 2025 | Supervised Fine-Tuning | [DrugAssist: a large language model for molecule optimization](https://academic.oup.com/bib/article/26/1/bbae693/7942355) | [Code](https://github.com/blazerye/DrugAssist) |
| **GeLLM³O** | arXiv 2025.02 | Supervised Fine-Tuning | [GeLLM³O: Generalizing Large Language Models for Multi-property Molecule Optimization](https://arxiv.org/pdf/2502.13398) | [Code](https://github.com/ninglab/GeLLMO) |
| **DrugLLM** | arXiv 2024.05 | Supervised Fine-Tuning | [DrugLLM: Open Large Language Model for Few-shot Molecule Generation](https://arxiv.org/html/2405.06690v1) | [N/A] |
| **LLM-Enhanced GA** | NeurIPS2024 Workshop | Supervised Fine-Tuning | [Small Molecule Optimization with Large Language Models](https://openreview.net/forum?id=p5VDaa8aIY) |[Code](https://github.com/yerevann/chemlactica) |
| **MolX-Enhanced LLM** | arXiv 2024.06 | Supervised Fine-Tuning | [MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension](https://arxiv.org/html/2406.06777v1) | [N/A] |
| **TOMG-Bench** | arXiv 2024.12 | Supervised Fine-Tuning | [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/pdf/2412.14642) | [Code](https://github.com/phenixace/TOMG-Bench) |
| **NatureLM**   | arXiv 2025.02 | Preference Tuning | [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/pdf/2502.07527) | [Code](https://naturelm.github.io/) |




## 📚 Datasets

A curated collection of popular datasets used in LLM-centric molecule generation, optimization, and understanding.

|Dataset|Last Update|Scale|Instruction|Pretraining|Benchmark|SMILES|IUPAC|Dock|Graph|3D|Ontology|Generation|Optimization|Other Tasks|Link|
|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--|:--|
|**PubChem** | 2025 | 119M | ✘ | ✔️ | ✘ | ✔️ | ✔️ | ✘ | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | Property Prediction & Biology | [Link](https://pubchem.ncbi.nlm.nih.gov/docs/downloads) |
|**ChEMBL** | 2024 | >20M | ✘ | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | ✔️ | ✘ | ✘ | ✔️ | ✔️ | ML Benchmark | [Link](https://ftp.ebi.ac.uk/pub/databases/chembl/ChEMBLdb/releases/chembl_35/) |
|**CrossDocked2020** | 2024 | 22.5M | ✘ | ✔️ | ✔️ | ✔️ | ✘ | ✔️ | ✘ | ✔️ | ✘ | ✘ | ✔️ | Docking | [Link](https://github.com/gnina/models/tree/master/data/CrossDocked2020) |
|**ZINC** | 2023 | >980M | ✘ | ✔️ | ✘ | ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | ✔️ | ✔️ | Ligand Discovery | [Link](https://wiki.docking.org/index.php?title=ZINC15:Getting_started#Downloading_SMILES) |
|**Dockstring** | 2022 | >260k | ✘ | ✔️ | ✔️ | ✔️ | ✘ | ✔️ | ✔️ | ✔️ | ✘ | ✔️ | ✔️ | Virtual Screening | [Link](https://figshare.com/articles/dataset/dockstring_dataset/16511577) |
|**ChEBI-20** | 2021 | 33k | ✘ | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | ✔️ | ✘ | ✔️ | ✔️ | ✘ | Captioning, Translation | [Link](https://github.com/cnedwards/text2mol/tree/master/data) |
|**OGBG-MolHIV** | 2020 | ~41k | ✘ | ✔️ | ✔️ | ✔️ | ✘ | ✘ | ✔️ | ✘ | ✘ | ✔️ | ✘ | Property Prediction | [Link](https://ogb.stanford.edu/docs/graphprop/#ogbg-mol) |
|**MOSES** | 2020 | ~1.9M | ✘ | ✘ | ✔️ | ✔️ | ✘ | ✘ | ✘ | ✘ | ✘ | ✔️ | ✘ | De Novo Design | [Link](https://github.com/molecularsets/moses) |
|**MoleculeNet** | 2019 | 700k | ✘ | ✘ | ✔️ | ✔️ | ✘ | ✘ | ✔️ | ✔️ | ✘ | ✔️ | ✔️ | ML Benchmark | [Link](https://github.com/deepchem/deepchem) |
|**QM9** | 2014 | 134k | ✘ | ✔️ | ✔️ | ✔️ | ✘ | ✘ | ✔️ | ✔️ | ✘ | ✔️ | ✔️ | QM/ML Modeling | [Link](https://figshare.com/collections/Quantum_chemistry_structures_and_properties_of_134_kilo_molecules/978904/5) |
|**TOMG-Bench** | 2025 | 5k | ✔️ | ✘ | ✔️ | ✔️ | ✘ | ✘ | ✔️ | ✔️ | ✔️ | ✔️ | ✔️ | Editing Tasks | [Link](https://github.com/phenixace/TOMG-Bench) |
|**MuMOInstruct** | 2025 | 873k | ✔️ | ✔️ | ✘ | ✔️ | ✘ | ✘ | ✘ | ✘ | ✘ | ✘ | ✘ | — | [Link](https://huggingface.co/datasets/NingLab/MuMOInstruct) |
|**ChemData** | 2024 | 7M | ✔️ | ✔️ | ✘ | ✔️ | ✘ | ✔️ | ✘ | ✘ | ✘ | ✔️ | ✔️ | Conversion & Reaction | [Link](https://huggingface.co/datasets/AI4Chem/ChemData700K) |
|**ChemBench** | 2024 | 4k | ✔️ | ✘ | ✔️ | ✔️ | ✘ | ✘ | ✘ | ✘ | ✘ | ✔️ | ✔️ | Virtual Screening | [Link](https://huggingface.co/datasets/jablonkagroup/ChemBench) |
|**Mol-Instructions** | 2024 | 2M | ✔️ | ✔️ | ✘ | ✔️ | ✘ | ✘ | ✘ | ✘ | ✘ | ✔️ | ✔️ | Retrosynthesis | [Link](https://huggingface.co/collections/zjunlp/mol-instructions-662e0b9435ab6df9593e8ea0) |
|**MolOpt-Instructions** | 2024 | 1M | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | ✘ | ✘ | ✘ | ✘ | ✘ | ✔️ | — | [Link](https://huggingface.co/datasets/blazerye/MolOpt-Instructions) |
|**L+M-24** | 2024 | 148k | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | ✘ | ✔️ | ✘ | ✘ | ✔️ | ✘ | Captioning | [Link](https://github.com/language-plus-molecules/LPM-24-Dataset) |
|**SMolInstruct** | 2024 | 3.3M | ✔️ | ✔️ | ✔️ | ✔️ | ✘ | ✘ | ✘ | ✘ | ✘ | ✔️ | ✘ | Captioning & Prediction | [Link](https://huggingface.co/datasets/osunlp/SMolInstruct/tree/main) |


## 📏 Evaluation Metrics

### 🧬 Structure-Based Metrics

These metrics assess the **chemical plausibility**, **similarity to references**, and **diversity** of generated or modified molecules.

#### 🔹 Validity & Similarity

- **Validity Rate**: Fraction of generated molecules that are chemically valid (e.g., parsable by RDKit).
- **EM (Exact Match)**: Checks if a generated molecular sequence is identical to a reference.
- **BLEU (Bilingual Evaluation Understudy)**: Measures n-gram overlap between generated and reference molecular sequences.
- **Levenshtein Distance**: Minimum number of single-character edits to convert one molecule string into another.
- **FTS (Fingerprint Tanimoto Similarity)**: Quantifies structural similarity based on molecular fingerprints (e.g., MACCS, RDKit, Morgan).
- **FCD (Fréchet ChemNet Distance)**: Measures dissimilarity between feature distributions of generated and reference molecules.

#### 🔹 Diversity & Uniqueness

- **Uniqueness**: Proportion of valid generated molecules that are distinct (e.g., Unique@1k, Unique@10k).
- **Novelty Rate**: Fraction of valid and unique molecules not present in the training set.
- **IntDiv (Internal Diversity)** / **NCircles**: Structural diversity within the generated molecular set.


### 🧪 Property-Based Metrics

These metrics evaluate whether a molecule satisfies specific **physicochemical** or **biological** property constraints.

#### 🔹 Single-Property Evaluation

- **LogP (Octanol-Water Partition Coefficient)**: Indicates molecular hydrophobicity.
- **QED (Quantitative Estimate of Drug-likeness)**: Heuristic score (0-1) for overall drug-likeness based on multiple properties.
- **TPSA (Topological Polar Surface Area)**: Sum of polar surface areas, relevant to hydrogen bonding.
- **SA Score (Synthetic Accessibility Score)**: Estimates synthetic feasibility (1 = easy, 10 = hard).

#### 🔹 Multi-Property Evaluation

- **Composite Score**: Weighted aggregation of multiple property metrics into a scalar.
- **Pareto Optimality**: A molecule is Pareto optimal if no property can improve without another worsening.
- **Success Rate under Constraints**: Proportion of generated molecules meeting all specified target thresholds.




