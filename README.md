# üöÄ Awesome-LLM-Guided-Molecule-Discovery
A collection of AWESOME things about Molecule Discovery with LLMs.

ü§ó Contributions to update new resources and articles are very welcome!

## ‚ùñ Contents 
- [Molecule Generation](#molecular-generation)
- [Molecule Optimization](#molecular-optimization)

 
## Molecule Generation

|Name|Year|Category|Paper|Code|
| :------------ |:---------------: |:---------------:| :---------------| :---------------|
| **LLM4GraphGen** | arXiv 2024.04 | In-Context Learning  | [Exploring the Potential of Large Language Models in Graph Generation](https://arxiv.org/abs/2403.14358) | [Code](https://github.com/SitaoLuan/LLM4Graph) |
| **MolReGPT** | TKDE | In-Context Learning  | [MolReGPT: Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective](https://arxiv.org/pdf/2306.06615) | [Code](https://github.com/phenixace/MolReGPT) |
| **FrontierX** | arXiv 2024.08 | In-Context Learning | [Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design](https://arxiv.org/pdf/2408.11866) | [N/A]) |
| **ICMA** | arXiv 2024.03 | Surpevised Fine-Tuning  | [Large Language Models are In-Context Molecule Learners](https://arxiv.org/pdf/2403.04197) | [N/A] |
| **Mol-instructions** | ICLR 2024 | Surpevised Fine-Tuning | [MOL-INSTRUCTIONS: A LARGE-SCALE BIOMOLECULAR INSTRUCTION DATASET FOR LLMS](https://arxiv.org/pdf/2306.08018) | [Code](https://github.com/zjunlp/Mol-Instructions) |
| **LlaSMol** | COLM 2024 | Surpevised Fine-Tuning | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/pdf/2403.04197) | [Code](https://osu-nlp-group.github.io/LLM4Chem/) |
| **ChemLLM** | arXiv 2024.04 | Surpevised Fine-Tuning | [ChemLLM: A Chemical Large Language Model](https://arxiv.org/pdf/2402.06852) | [N/A] |
| **MolReFlect** | arXiv 2024.11 | Surpevised Fine-Tuning | [MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts](https://arxiv.org/pdf/2411.14721) | [Code](https://github.com/vllm-project/vllm) |
| **ChatMol** | arXiv 2025.02 | Surpevised Fine-Tuning | [ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model](https://arxiv.org/pdf/2502.19794) | [Code](https://github.com/ChatMol/ChatMol) |
| **PEIT-LLM**   | arXiv 2024.12 | Surpevised Fine-Tuning | [Property Enhanced Instruction Tuning for Multi-task Molecule Generation with LLMs](https://arxiv.org/pdf/2412.18084) | [Code](https://github.com/chenlong164/PEIT) |
| **NatureLM**   | arXiv 2025.02 | Surpevised Fine-Tuning | [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/pdf/2502.07527) | [Code](https://naturelm.github.io/) |
| **SynLlama**   | arXiv 2025.03 | Surpevised Fine-Tuning | [SynLlama: Generating Synthesizable Molecules and Analogs with LLMs](https://arxiv.org/pdf/2503.12602) |[N/A]|
| **TOMG-Bench** | arXiv 2024.12 | Surpevised Fine-Tuning | [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/pdf/2412.14642) | [Code](https://github.com/phenixace/TOMG-Bench) |
| **UniMoT** | arXiv 2024.08 | Surpevised Fine-Tuning | [UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation](https://arxiv.org/pdf/2408.00863) | [Code](https://uni-mot.github.io/) |
| **Div-SFT** | arXiv 2024.10 | Preference Tuning | [Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity](https://arxiv.org/pdf/2410.03138) | [N/A] |
| **Mol-MoE** | arXiv 2025.02 | Preference Tuning | [Mol-MoE: Training Preference-Guided Routers for Molecule Generation](https://arxiv.org/pdf/2502.05633) | [Code](https://github.com/ddidacus/mol-moe) |
| **SmileyLLama**| NeurIPS2024 Workshop | Preference Tuning | [SmileyLlama: Modifying LLMs for Directed Chemical Space Exploration](https://arxiv.org/pdf/2409.02231) |[N/A] |
| **ALMol** | ACL2024 Workshop | Preference Tuning | [ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation](https://arxiv.org/abs/2405.08619) |[N/A]|
| **Less for More** | arXiv 2024.05 | Preference Tuning | [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/pdf/2405.13984) | [N/A] |
| **Mol-LLM** | arXiv 2025.02 | Preference Tuning | [Mol-llm:Generalist molecular llm with improved graph utilization](https://arxiv.org/pdf/2502.02810)|[N/A]|






## Molecule Optimization

|Name|Year|Category|Paper|Code|
| :------------ |:---------------: |:---------------:| :---------------| :---------------| 
| **MOLLEO** | ICLR2025 Workshop | Zero-Shot Prompting | [EFFICIENT EVOLUTIONARY SEARCH OVER CHEMICAL SPACE WITH LARGE LANGUAGE MODELS](https://openreview.net/pdf?id=awWiNvQwf3) | [Code](https://github.com/zoom-wang112358/MOLLEO) |
| **LLM-MDE** | JCIM 2024 | Zero-Shot Prompting | [Large Language Models as Molecular Design Engines](https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.4c01396) | [N/A] |
| **CIDD** | arXiv 2025.03 | In-Context Learning | [Pushing the boundaries of Structure-Based Drug Design through Collaborative Intelligence Drug Design](https://arxiv.org/pdf/2503.01376) | [N/A] |
| **LLM-EO** | arXiv 2025.03 | In-Context Learning | [Generative Design of Functional Metal Complexes Utilizing the Internal Knowledge of Large Language Models](https://arxiv.org/pdf/2410.18136)| [Code](https://github.com/deepprinciple/llmeo) |
| **MOLLM** | arXiv 2025.03 | In-Context Learning | [MOLLM: Multi-Objective Large Language Model for Molecular Design ‚Äì Optimizing with Experts](https://arxiv.org/pdf/2503.01376) | [N/A] |
| **ChatDrug** | ICLR 2024 | In-Context Learning  | [Conversational Drug Editing Using Retrieval and Domain Feedback](https://openreview.net/pdf?id=yRrPfKyJQ2) | [Code](https://github.com/chao1224/ChatDrug) |
| **Re^2DF** | arXiv 2024.10* | In-Context Learning | [Utilizing Large Language Models in an Iterative Paradigm with Domain Feedback for Molecule Optimization](https://arxiv.org/pdf/2410.13147) | [Code](https://github.com/lhkhiem28/Re2DF) |
| **BOPRO** | ICLR 2025 | In-Context Learning | [Searching for Optimal Solutions with LLMs via Bayesian Optimization](https://openreview.net/pdf?id=aVfDrl7xDV) | [Code](https://github.com/amazon-science/BOPRO-ICLR-2025) |
| **MultiMol** | arXiv 2025 | Supervised Fine-Tuning | [Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization](https://arxiv.org/html/2503.03503) | [Code](https://github.com/jiajunyu1999/LLM4Drug) |
| **DrugAssist** | Briefings in Bioinformatics 2025 | Supervised Fine-Tuning | [DrugAssist: a large language model for molecule optimization](https://academic.oup.com/bib/article/26/1/bbae693/7942355) | [Code](https://github.com/blazerye/DrugAssist) |
| **GeLLM¬≥O** | arXiv 2025 | Supervised Fine-Tuning | [GeLLM¬≥O: Generalizing Large Language Models for Multi-property Molecule Optimization](https://arxiv.org/pdf/2502.13398) | [Code](https://github.com/ninglab/GeLLMO) |
| **DrugLLM** | CoRR 2024 | Supervised Fine-Tuning | [DrugLLM: Open Large Language Model for Few-shot Molecule Generation](https://arxiv.org/html/2405.06690v1) | [N/A] |
| **LLM-Enhanced GA** | NeurIPS2024 Workshop | Supervised Fine-Tuning | [Small Molecule Optimization with Large Language Models](https://openreview.net/forum?id=p5VDaa8aIY) |[Code](https://github.com/yerevann/chemlactica) |
| **MolX-Enhanced LLM** | arXiv 2024 | Supervised Fine-Tuning | [MolX: Enhancing Large Language Models for Molecular Learning with A Multi-Modal Extension](https://arxiv.org/html/2406.06777v1) | [N/A] |
| **TOMG-Bench** | arXiv 2024 | Supervised Fine-Tuning | [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/pdf/2412.14642) | [Code](https://github.com/phenixace/TOMG-Bench) |
| **NatureLM**   | arXiv 2025.02 | Preference Tuning | [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/pdf/2502.07527) | [Code](https://naturelm.github.io/) |

