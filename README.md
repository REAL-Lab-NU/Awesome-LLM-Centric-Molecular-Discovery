# üöÄ Awesome-LLM-Guided-Molecule-Discovery
A collection of AWESOME things about Molecule Discovery with LLMs.

ü§ó Contributions to update new resources and articles are very welcome!

## ‚ùñ Contents 
- [Molecule Generation](#molecular-generation)
- [Molecule Optimization](#molecular-optimization)

 
## Molecule Generation

|Name|Year|Category|Paper|Code|
| :------------ |:---------------: |:---------------:| :---------------| :---------------|
| **LLM4GraphGen** | arXiv 2024.04 | In-Context Learning  | [Exploring the Potential of Large Language Models in Graph Generation](https://arxiv.org/abs/2403.14358) | [N/A]) |
| **MolReGPT** | TKDE | In-Context Learning  | [MolReGPT: Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective](https://arxiv.org/pdf/2306.06615) | [Code](https://github.com/phenixace/MolReGPT?tab=readme-ov-file) |
| **FrontierX** | arXiv 2024.08 | In-Context Learning | [Crossing New Frontiers: Knowledge-Augmented Large Language Model Prompting for Zero-Shot Text-Based De Novo Molecule Design](https://arxiv.org/pdf/2408.11866) | [N/A]) |
| **ICMA** | arXiv 2024.03 | Surpevised Fine-Tuning  | [Large Language Models are In-Context Molecule Learners](https://arxiv.org/pdf/2403.04197) | [N/A] |
| **Mol-instructions** | ICLR 2024 | Surpevised Fine-Tuning | [MOL-INSTRUCTIONS: A LARGE-SCALE BIOMOLECULAR INSTRUCTION DATASET FOR LLMS](https://arxiv.org/pdf/2306.08018) | [N/A] |
| **LlaSMol** | COLM 2024 | Surpevised Fine-Tuning | [LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset](https://arxiv.org/pdf/2403.04197) | [N/A] |
| **ChemLLM** | arXiv 2024.04 | Surpevised Fine-Tuning | [ChemLLM: A Chemical Large Language Model](https://arxiv.org/pdf/2402.06852) | [N/A] |
| **MolReFlect** | arXiv 2024.11 | Surpevised Fine-Tuning | [MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts](https://arxiv.org/pdf/2411.14721) | [N/A] |
| **ChatMol** | arXiv 2025.02 | Surpevised Fine-Tuning | [ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model](https://arxiv.org/pdf/2502.19794) | [N/A] |
| **PEIT-LLM**   | arXiv 2024.12 | Surpevised Fine-Tuning | [Property Enhanced Instruction Tuning for Multi-task Molecule Generation with LLMs](https://arxiv.org/abs/2412.18084) | [Code](https://github.com/chenlong164/PEIT) |
| **NatureLM**   | arXiv 2025.02 | Surpevised Fine-Tuning | [NatureLM: Deciphering the Language of Nature for Scientific Discovery](https://arxiv.org/abs/2502.07527) | [Code](https://naturelm.github.io/) |
| **SynLlama**   | arXiv 2025.03 | Surpevised Fine-Tuning | [SynLlama: Generating Synthesizable Molecules and Analogs with LLMs](https://arxiv.org/abs/2503.12602) | [Code](https://github.com/THGLab/SynLlama) |
| **TOMG-Bench** | arXiv 2024.12 | Surpevised Fine-Tuning | [TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation](https://arxiv.org/abs/2412.14642) | [Code](https://github.com/phenixace/TOMG-Bench) |
| **UniMoT**     | arXiv 2024.08 | Surpevised Fine-Tuning | [UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation](https://arxiv.org/abs/2408.00863) | [Code](https://uni-mot.github.io/) |
| **Mol-MoE**    | arXiv 2025.02 | Preference Tuning | [Mol-MoE: Training Preference-Guided Routers for Molecule Generation](https://arxiv.org/abs/2502.05633) | [Code](https://github.com/ddidacus/mol-moe) |
| **SmileyLlama**| arXiv 2024.09 | Preference Tuning | [SmileyLlama: Modifying LLMs for Directed Chemical Space Exploration](https://arxiv.org/abs/2409.02231) |[N/A] |
| **ALMol** | arXiv 2024.05 | Preference Tuning | [ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation](https://arxiv.org/abs/2405.08619) |[N/A]|
| **Less for More** | arXiv 2024.05 | Preference Tuning | [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/abs/2405.13984) | [OpenReview](https://openreview.net/forum?id=lessformore2024) |
| **MOLLEO** | arXiv 2025.02 | Preference Tuning | [MOLLEO: Multi-Objective Large Language Model for Molecular Design](https://arxiv.org/abs/2502.12845) | [Code](https://molleo.github.io/) |
| **CIDD** | arXiv 2025.03 | Preference Tuning | [Pushing the boundaries of Structure-Based Drug Design through Collaborative Intelligence Drug Design](https://arxiv.org/abs/2503.01376) | [N/A] |
| **MOLLM** | arXiv 2025.02 | Preference Tuning | [MOLLM: Multi-Objective Large Language Model for Molecular Design](https://arxiv.org/abs/2502.12845) | [N/A] |







## Molecule Optimization

|Name|Year|Category|Paper|Code|
| :------------ |:---------------: |:---------------:| :---------------| :---------------| 
| **ChatDrug** | ICLR 2024 | In-Context Learning  | [Conversational Drug Editing Using Retrieval and Domain Feedback](https://openreview.net/pdf?id=yRrPfKyJQ2) | [Code](https://github.com/chao1224/ChatDrug) |
